{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "epochs = 200"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Tagging sentences:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6149bf0c5fcc4a8793bb159dba560d0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Parsing tagged sentences:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc60b5b80b5c4b87b8161dd50150154c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Parse trees to diagrams:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a25ffa45f58e4b8c9ff5d62647c40385"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from lambeq import BobcatParser\n",
    "import numpy as np\n",
    "\n",
    "train_data = ['cat runs on land']\n",
    "train_labels = [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
    "\n",
    "parser = BobcatParser()\n",
    "\n",
    "train_diagrams = parser.sentences2diagrams(train_data)\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "def CEL(y_hat, y):\n",
    "    flattened_y_hat = y_hat.flatten()\n",
    "    return float(loss(torch.tensor(flattened_y_hat), torch.tensor(y[0])))\n",
    "\n",
    "\n",
    "def acc(y_hat, y):\n",
    "    a=y_hat.flatten()\n",
    "    #b=y_hat\n",
    "    max_index = np.argmax(a)\n",
    "    new_arr = np.zeros(a.shape)\n",
    "    new_arr[max_index] = 1\n",
    "    if np.array_equal(new_arr, y[0]):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "from lambeq import AtomicType, IQPAnsatz\n",
    "\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "P = AtomicType.PREPOSITIONAL_PHRASE\n",
    "ansatz = IQPAnsatz(ob_map={N: 1, S: 4, P:1}, n_layers=3, n_single_qubit_params=6)\n",
    "train_circuits = [ansatz(diagram) for diagram in train_diagrams]\n",
    "\n",
    "\n",
    "from lambeq import NumpyModel\n",
    "model = NumpyModel.from_diagrams(train_circuits)\n",
    "from lambeq import QuantumTrainer, SPSAOptimizer\n",
    "\n",
    "# here you can play around with hyperparameters\n",
    "trainer = QuantumTrainer(\n",
    "    model,\n",
    "    loss_function=CEL,\n",
    "    epochs=epochs,\n",
    "    optimizer=SPSAOptimizer,\n",
    "    optim_hyperparams={'a': 0.2, 'c': 0.06, 'A':0.01*epochs},\n",
    "    evaluate_functions={'acc': acc},\n",
    "    evaluate_on_train=True,\n",
    "    verbose = 'text',\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "from lambeq import Dataset\n",
    "\n",
    "train_dataset = Dataset(\n",
    "            train_circuits,\n",
    "            train_labels,\n",
    "            batch_size=1,\n",
    "            shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:    train/loss: 2.8305   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 2:    train/loss: 2.8097   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 3:    train/loss: 2.8107   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 4:    train/loss: 2.8019   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 5:    train/loss: 2.7670   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 6:    train/loss: 2.6856   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 7:    train/loss: 2.7324   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 8:    train/loss: 2.6878   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 9:    train/loss: 2.6023   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 10:   train/loss: 2.6023   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 11:   train/loss: 2.5546   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 12:   train/loss: 2.6135   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 13:   train/loss: 2.6578   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 14:   train/loss: 2.6192   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 15:   train/loss: 2.6488   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 16:   train/loss: 2.5757   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 17:   train/loss: 2.6372   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 18:   train/loss: 2.7830   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 19:   train/loss: 2.7203   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 20:   train/loss: 2.7203   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 21:   train/loss: 2.7693   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 22:   train/loss: 2.6222   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 23:   train/loss: 2.6622   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 24:   train/loss: 2.7162   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 25:   train/loss: 2.7541   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 26:   train/loss: 2.6577   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 27:   train/loss: 2.6659   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 28:   train/loss: 2.6736   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 29:   train/loss: 2.6763   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 30:   train/loss: 2.5944   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 31:   train/loss: 2.5309   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 32:   train/loss: 2.5288   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 33:   train/loss: 2.5147   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 34:   train/loss: 2.4518   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 35:   train/loss: 2.5081   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 36:   train/loss: 2.4565   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 37:   train/loss: 2.6417   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 38:   train/loss: 2.5696   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 39:   train/loss: 2.5775   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 40:   train/loss: 2.6565   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 41:   train/loss: 2.5919   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 42:   train/loss: 2.6112   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 43:   train/loss: 2.6000   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 44:   train/loss: 2.4956   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 45:   train/loss: 2.4797   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 46:   train/loss: 2.4538   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 47:   train/loss: 2.4357   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 48:   train/loss: 2.4244   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 49:   train/loss: 2.5021   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 50:   train/loss: 2.4536   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 51:   train/loss: 2.3993   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 52:   train/loss: 2.3722   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 53:   train/loss: 2.4197   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 54:   train/loss: 2.4094   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 55:   train/loss: 2.3759   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 56:   train/loss: 2.3146   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 57:   train/loss: 2.2556   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 58:   train/loss: 2.3453   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 59:   train/loss: 2.3303   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 60:   train/loss: 2.3379   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 61:   train/loss: 2.3118   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 62:   train/loss: 2.4032   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 63:   train/loss: 2.3650   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 64:   train/loss: 2.4283   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 65:   train/loss: 2.4078   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 66:   train/loss: 2.3336   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 67:   train/loss: 2.3169   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 68:   train/loss: 2.4140   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 69:   train/loss: 2.5317   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 70:   train/loss: 2.3692   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 71:   train/loss: 2.3086   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 72:   train/loss: 2.3420   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 73:   train/loss: 2.3118   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 74:   train/loss: 2.3528   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 75:   train/loss: 2.2602   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 76:   train/loss: 2.3622   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 77:   train/loss: 2.3920   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 78:   train/loss: 2.5056   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 79:   train/loss: 2.6458   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 80:   train/loss: 2.4738   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 81:   train/loss: 2.6276   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 82:   train/loss: 2.5240   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 83:   train/loss: 2.5466   valid/loss: -----   train/acc: 0.0000   valid/acc: -----\n",
      "Epoch 84:   train/loss: 2.5178   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 85:   train/loss: 2.5249   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 86:   train/loss: 2.4613   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 87:   train/loss: 2.4468   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 88:   train/loss: 2.4313   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 89:   train/loss: 2.4484   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 90:   train/loss: 2.5149   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 91:   train/loss: 2.4295   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 92:   train/loss: 2.5251   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 93:   train/loss: 2.5020   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 94:   train/loss: 2.5173   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 95:   train/loss: 2.4597   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 96:   train/loss: 2.4826   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 97:   train/loss: 2.3385   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 98:   train/loss: 2.3017   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 99:   train/loss: 2.3143   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 100:  train/loss: 2.3340   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 101:  train/loss: 2.2541   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 102:  train/loss: 2.3089   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 103:  train/loss: 2.2530   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 104:  train/loss: 2.1934   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 105:  train/loss: 2.2064   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 106:  train/loss: 2.1351   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 107:  train/loss: 2.2335   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 108:  train/loss: 2.1701   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 109:  train/loss: 2.1163   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 110:  train/loss: 2.1411   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 111:  train/loss: 2.0744   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 112:  train/loss: 2.1283   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 113:  train/loss: 2.1386   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 114:  train/loss: 2.1289   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 115:  train/loss: 2.1188   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 116:  train/loss: 2.0919   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 117:  train/loss: 2.1192   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 118:  train/loss: 2.0730   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 119:  train/loss: 2.1017   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 120:  train/loss: 2.0546   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 121:  train/loss: 2.1243   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 122:  train/loss: 2.1199   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 123:  train/loss: 2.0523   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 124:  train/loss: 2.0406   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 125:  train/loss: 2.0652   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 126:  train/loss: 2.0415   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 127:  train/loss: 2.0418   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 128:  train/loss: 2.0437   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 129:  train/loss: 2.0640   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 130:  train/loss: 2.0882   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 131:  train/loss: 2.0419   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 132:  train/loss: 2.0803   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 133:  train/loss: 2.0498   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 134:  train/loss: 2.0907   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 135:  train/loss: 2.0373   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 136:  train/loss: 2.0894   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 137:  train/loss: 2.0478   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 138:  train/loss: 2.0535   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 139:  train/loss: 2.0240   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 140:  train/loss: 2.0580   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 141:  train/loss: 2.0426   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 142:  train/loss: 2.0649   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 143:  train/loss: 2.0313   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 144:  train/loss: 2.0783   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 145:  train/loss: 2.0811   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 146:  train/loss: 2.0874   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 147:  train/loss: 2.0482   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 148:  train/loss: 2.0584   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 149:  train/loss: 2.1142   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 150:  train/loss: 2.0007   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 151:  train/loss: 2.0859   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 152:  train/loss: 2.0598   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 153:  train/loss: 2.0070   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 154:  train/loss: 2.0502   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 155:  train/loss: 2.0736   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 156:  train/loss: 2.0138   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 157:  train/loss: 2.0901   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 158:  train/loss: 2.0260   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 159:  train/loss: 2.0309   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 160:  train/loss: 2.0223   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 161:  train/loss: 2.0487   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 162:  train/loss: 2.0578   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 163:  train/loss: 2.0594   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 164:  train/loss: 1.9906   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 165:  train/loss: 1.9997   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 166:  train/loss: 2.0564   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 167:  train/loss: 2.0149   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 168:  train/loss: 1.9935   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 169:  train/loss: 1.9978   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 170:  train/loss: 1.9889   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 171:  train/loss: 2.0568   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 172:  train/loss: 2.0003   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 173:  train/loss: 2.0087   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 174:  train/loss: 1.9925   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 175:  train/loss: 2.0469   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 176:  train/loss: 2.0383   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 177:  train/loss: 2.0183   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 178:  train/loss: 2.1608   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 179:  train/loss: 2.0321   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 180:  train/loss: 1.9738   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 181:  train/loss: 2.0870   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 182:  train/loss: 2.1015   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 183:  train/loss: 2.0012   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 184:  train/loss: 2.0486   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 185:  train/loss: 2.0501   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 186:  train/loss: 2.0423   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 187:  train/loss: 2.1018   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 188:  train/loss: 2.0504   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 189:  train/loss: 2.0376   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 190:  train/loss: 2.0187   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 191:  train/loss: 2.0453   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 192:  train/loss: 2.1446   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 193:  train/loss: 2.0759   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 194:  train/loss: 2.0647   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 195:  train/loss: 2.0253   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 196:  train/loss: 2.1933   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 197:  train/loss: 2.0002   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 198:  train/loss: 2.0494   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 199:  train/loss: 2.0454   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "Epoch 200:  train/loss: 2.0900   valid/loss: -----   train/acc: 1.0000   valid/acc: -----\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(train_dataset, logging_step=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.12055281, 0.61578376, 0.00976868, 0.03333487, 0.0007148 ,\n       0.01370844, 0.00289223, 0.06434937, 0.01203994, 0.10307202,\n       0.00219166, 0.00586063, 0.00156769, 0.00437874, 0.00363166,\n       0.0061527 ])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(train_circuits).flatten()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "'''from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "service = QiskitRuntimeService(channel=\"ibm_quantum\")\n",
    "backend = service.get_backend(\"ibm_washington\")'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from pytket.extensions.qiskit import set_ibmq_config\n",
    "set_ibmq_config(hub= \"ibm-q-unibw\", group=\"external-student\", project=\"qnlp\", ibmq_api_token=\"7c4a348dd83600545d560523c09e0cb05ce80191c267fcce23eba6e11cb8af4b5d446ab4182525881e9e3195a383a1006f92af6a4084d35b05e27f8456321d80\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "Tagging sentences:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "617dd9d49aed4dcc84c96c238d65ff9d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Parsing tagged sentences:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "814e9e3f8dea4b35966039278e199d90"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Parse trees to diagrams:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17ae9c5d1c48447788c91564dab32224"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from lambeq import BobcatParser\n",
    "import numpy as np\n",
    "\n",
    "train_data = ['cat runs on land']\n",
    "train_labels = [[0., 1.]]\n",
    "\n",
    "parser = BobcatParser()\n",
    "\n",
    "train_diagrams = parser.sentences2diagrams(train_data)\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "def CEL(y_hat, y):\n",
    "    flattened_y_hat = y_hat.flatten()\n",
    "    return float(loss(torch.tensor(flattened_y_hat), torch.tensor(y[0])))\n",
    "\n",
    "\n",
    "def acc(y_hat, y):\n",
    "    a=y_hat.flatten()\n",
    "    #b=y_hat\n",
    "    max_index = np.argmax(a)\n",
    "    new_arr = np.zeros(a.shape)\n",
    "    new_arr[max_index] = 1\n",
    "    if np.array_equal(new_arr, y[0]):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "from lambeq import AtomicType, IQPAnsatz\n",
    "\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "P = AtomicType.PREPOSITIONAL_PHRASE\n",
    "ansatz = IQPAnsatz(ob_map={N: 1, S: 1, P:1}, n_layers=1, n_single_qubit_params=1)\n",
    "train_circuits = [ansatz(diagram) for diagram in train_diagrams]\n",
    "\n",
    "from pytket.extensions.qiskit import IBMQBackend, IBMQEmulatorBackend\n",
    "from lambeq import TketModel\n",
    "backend = IBMQBackend(\"ibmq_kolkata\")\n",
    "backend_config = {\n",
    "    'backend': backend,\n",
    "    'compilation': backend.default_compilation_pass(2),\n",
    "    'shots': 8192\n",
    "}\n",
    "model = TketModel.from_diagrams(train_circuits, backend_config=backend_config)\n",
    "\n",
    "\n",
    "\n",
    "from lambeq import QuantumTrainer, SPSAOptimizer\n",
    "\n",
    "# here you can play around with hyperparameters\n",
    "trainer = QuantumTrainer(\n",
    "    model,\n",
    "    loss_function=CEL,\n",
    "    epochs=epochs,\n",
    "    optimizer=SPSAOptimizer,\n",
    "    optim_hyperparams={'a': 0.2, 'c': 0.06, 'A':0.01*epochs},\n",
    "    evaluate_functions={'acc': acc},\n",
    "    evaluate_on_train=True,\n",
    "    verbose = 'text',\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "from lambeq import Dataset\n",
    "\n",
    "train_dataset = Dataset(\n",
    "            train_circuits,\n",
    "            train_labels,\n",
    "            batch_size=1,\n",
    "            shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "ResultHandleTypeError",
     "evalue": "ResultHandle(<bound method JobV1.job_id of <RuntimeJob('cgvrmkfgbjvrfp7g2tu0', 'sampler')>>, 0, 9, 'null') does not match expected identifier types (<class 'str'>, <class 'int'>, <class 'int'>, <class 'str'>)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mResultHandleTypeError\u001B[0m                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogging_step\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/lambeq/training/quantum_trainer.py:199\u001B[0m, in \u001B[0;36mQuantumTrainer.fit\u001B[0;34m(self, train_dataset, val_dataset, evaluation_step, logging_step)\u001B[0m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    192\u001B[0m         train_dataset: Dataset,\n\u001B[1;32m    193\u001B[0m         val_dataset: Dataset \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    194\u001B[0m         evaluation_step: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m    195\u001B[0m         logging_step: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    197\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39m_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevaluation_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogging_step\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39m_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/lambeq/training/trainer.py:375\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, train_dataset, val_dataset, evaluation_step, logging_step)\u001B[0m\n\u001B[1;32m    373\u001B[0m step \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    374\u001B[0m x, y_label \u001B[38;5;241m=\u001B[39m batch\n\u001B[0;32m--> 375\u001B[0m y_hat, loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate_on_train\n\u001B[1;32m    377\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate_functions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    378\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m metr, func \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate_functions\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/lambeq/training/quantum_trainer.py:163\u001B[0m, in \u001B[0;36mQuantumTrainer.training_step\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Perform a training step.\u001B[39;00m\n\u001B[1;32m    150\u001B[0m \n\u001B[1;32m    151\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    160\u001B[0m \n\u001B[1;32m    161\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39m_clear_predictions()\n\u001B[0;32m--> 163\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    164\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39m_train_predictions[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_costs\u001B[38;5;241m.\u001B[39mappend(loss)\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/lambeq/training/spsa_optimizer.py:141\u001B[0m, in \u001B[0;36mSPSAOptimizer.backward\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    139\u001B[0m xplus \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproject(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mck \u001B[38;5;241m*\u001B[39m delta)\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mweights \u001B[38;5;241m=\u001B[39m xplus\n\u001B[0;32m--> 141\u001B[0m y0 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdiagrams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    142\u001B[0m loss0 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn(y0, targets)\n\u001B[1;32m    144\u001B[0m xminus \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproject(x \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mck \u001B[38;5;241m*\u001B[39m delta)\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/lambeq/training/quantum_model.py:146\u001B[0m, in \u001B[0;36mQuantumModel.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 146\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_training:\n\u001B[1;32m    148\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log_prediction(out)\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/lambeq/training/tket_model.py:133\u001B[0m, in \u001B[0;36mTketModel.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: \u001B[38;5;28mlist\u001B[39m[Diagram]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[1;32m    116\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Perform default forward pass of a lambeq quantum model.\u001B[39;00m\n\u001B[1;32m    117\u001B[0m \n\u001B[1;32m    118\u001B[0m \u001B[38;5;124;03m    In case of a different datapoint (e.g. list of tuple) or\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    131\u001B[0m \n\u001B[1;32m    132\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_diagram_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/lambeq/training/tket_model.py:102\u001B[0m, in \u001B[0;36mTketModel.get_diagram_output\u001B[0;34m(self, diagrams)\u001B[0m\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWeights and/or symbols not initialised. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     96\u001B[0m                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInstantiate through \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     97\u001B[0m                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`TketModel.from_diagrams()` first, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     98\u001B[0m                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthen call `initialise_weights()`, or load \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     99\u001B[0m                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfrom pre-trained checkpoint.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    101\u001B[0m lambdified_diagrams \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_lambda(d) \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m diagrams]\n\u001B[0;32m--> 102\u001B[0m tensors \u001B[38;5;241m=\u001B[39m \u001B[43mCircuit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    103\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mdiag_f\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdiag_f\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlambdified_diagrams\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackend_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_randint\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbackend\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[1;32m    108\u001B[0m \u001B[38;5;66;03m# discopy evals a single diagram into a single result\u001B[39;00m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;66;03m# and not a list of results\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/discopy/quantum/circuit.py:288\u001B[0m, in \u001B[0;36mCircuit.eval\u001B[0;34m(self, backend, mixed, contractor, *others, **params)\u001B[0m\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(box)(box\u001B[38;5;241m.\u001B[39mdom, box\u001B[38;5;241m.\u001B[39mcod, box\u001B[38;5;241m.\u001B[39marray \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m0\u001B[39mj)\n\u001B[1;32m    287\u001B[0m circuits \u001B[38;5;241m=\u001B[39m [circuit\u001B[38;5;241m.\u001B[39mto_tk() \u001B[38;5;28;01mfor\u001B[39;00m circuit \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28mself\u001B[39m, ) \u001B[38;5;241m+\u001B[39m others]\n\u001B[0;32m--> 288\u001B[0m results, counts \u001B[38;5;241m=\u001B[39m [], \u001B[43mcircuits\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_counts\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcircuits\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbackend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbackend\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, circuit \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(circuits):\n\u001B[1;32m    291\u001B[0m     n_bits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(circuit\u001B[38;5;241m.\u001B[39mpost_processing\u001B[38;5;241m.\u001B[39mdom)\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/discopy/quantum/tk.py:136\u001B[0m, in \u001B[0;36mCircuit.get_counts\u001B[0;34m(self, backend, *others, **params)\u001B[0m\n\u001B[1;32m    133\u001B[0m         compilation\u001B[38;5;241m.\u001B[39mapply(circuit)\n\u001B[1;32m    134\u001B[0m handles \u001B[38;5;241m=\u001B[39m backend\u001B[38;5;241m.\u001B[39mprocess_circuits(\n\u001B[1;32m    135\u001B[0m     (\u001B[38;5;28mself\u001B[39m, ) \u001B[38;5;241m+\u001B[39m others, n_shots\u001B[38;5;241m=\u001B[39mn_shots, seed\u001B[38;5;241m=\u001B[39mseed)\n\u001B[0;32m--> 136\u001B[0m counts \u001B[38;5;241m=\u001B[39m [backend\u001B[38;5;241m.\u001B[39mget_result(h)\u001B[38;5;241m.\u001B[39mget_counts() \u001B[38;5;28;01mfor\u001B[39;00m h \u001B[38;5;129;01min\u001B[39;00m handles]\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m normalize:\n\u001B[1;32m    138\u001B[0m     counts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(probs_from_counts, counts))\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/discopy/quantum/tk.py:136\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    133\u001B[0m         compilation\u001B[38;5;241m.\u001B[39mapply(circuit)\n\u001B[1;32m    134\u001B[0m handles \u001B[38;5;241m=\u001B[39m backend\u001B[38;5;241m.\u001B[39mprocess_circuits(\n\u001B[1;32m    135\u001B[0m     (\u001B[38;5;28mself\u001B[39m, ) \u001B[38;5;241m+\u001B[39m others, n_shots\u001B[38;5;241m=\u001B[39mn_shots, seed\u001B[38;5;241m=\u001B[39mseed)\n\u001B[0;32m--> 136\u001B[0m counts \u001B[38;5;241m=\u001B[39m [\u001B[43mbackend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mget_counts() \u001B[38;5;28;01mfor\u001B[39;00m h \u001B[38;5;129;01min\u001B[39;00m handles]\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m normalize:\n\u001B[1;32m    138\u001B[0m     counts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(probs_from_counts, counts))\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/pytket/extensions/qiskit/backends/ibm.py:504\u001B[0m, in \u001B[0;36mIBMQBackend.get_result\u001B[0;34m(self, handle, **kwargs)\u001B[0m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_result\u001B[39m(\u001B[38;5;28mself\u001B[39m, handle: ResultHandle, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: KwargTypes) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BackendResult:\n\u001B[1;32m    500\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    501\u001B[0m \u001B[38;5;124;03m    See :py:meth:`pytket.backends.Backend.get_result`.\u001B[39;00m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;124;03m    Supported kwargs: `timeout`, `wait`.\u001B[39;00m\n\u001B[1;32m    503\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 504\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_handle_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    505\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m handle \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cache:\n\u001B[1;32m    506\u001B[0m         cached_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cache[handle]\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/pytket/backends/backend.py:238\u001B[0m, in \u001B[0;36mBackend._check_handle_type\u001B[0;34m(self, reshandle)\u001B[0m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Check a result handle is valid for this backend, raises TypeError if not.\u001B[39;00m\n\u001B[1;32m    230\u001B[0m \n\u001B[1;32m    231\u001B[0m \u001B[38;5;124;03m:param reshandle: Handle to check\u001B[39;00m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;124;03m:type reshandle: ResultHandle\u001B[39;00m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;124;03m:raises TypeError: Types of handle identifiers don't match those of backend.\u001B[39;00m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mlen\u001B[39m(reshandle) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result_id_type)) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28misinstance\u001B[39m(idval, ty) \u001B[38;5;28;01mfor\u001B[39;00m idval, ty \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(reshandle, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result_id_type)\n\u001B[1;32m    237\u001B[0m ):\n\u001B[0;32m--> 238\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResultHandleTypeError(\n\u001B[1;32m    239\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{0!r}\u001B[39;00m\u001B[38;5;124m does not match expected identifier types \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    240\u001B[0m             reshandle, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result_id_type\n\u001B[1;32m    241\u001B[0m         )\n\u001B[1;32m    242\u001B[0m     )\n",
      "\u001B[0;31mResultHandleTypeError\u001B[0m: ResultHandle(<bound method JobV1.job_id of <RuntimeJob('cgvrmkfgbjvrfp7g2tu0', 'sampler')>>, 0, 9, 'null') does not match expected identifier types (<class 'str'>, <class 'int'>, <class 'int'>, <class 'str'>)"
     ]
    }
   ],
   "source": [
    "trainer.fit(train_dataset, logging_step=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
