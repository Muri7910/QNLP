{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-13T16:29:33.851464Z",
     "start_time": "2023-05-13T16:29:33.847163Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# High number of epochs, change for faster results\n",
    "import numpy as np\n",
    "\n",
    "# only a batch size of 1 works with hinge loss right now, with bce loss a larger batch size works, but in general a smaller batch size leads to better convergence, but still could theoretically introduce more noise\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1\n",
    "SEED = 2\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T16:29:33.922041Z",
     "start_time": "2023-05-13T16:29:33.853488Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# this sample data is found in folder data\n",
    "def read_raw_data(filename):\n",
    "    raw_data = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            raw_data.append((line[:-2], line[-2]))\n",
    "    return raw_data\n",
    "raw_train_data = read_raw_data('data/bin_class_86_train_data.txt')\n",
    "raw_dev_data = read_raw_data('data/bin_class_86_dev_data_1.txt')\n",
    "#raw_test_data = read_raw_data('data/bin_class_23_test_data.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T16:29:34.025887Z",
     "start_time": "2023-05-13T16:29:33.927965Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "grammar_classes = {\n",
    "    'nouns': [\"dog\", \"cat\", \"mouse\", \"fox\", \"chicken\", \"grain\", \"cheese\", \"bone\", \"fish\", \"whale\", \"seal\", \"krill\", \"water\", \"land\"],\n",
    "    'transitive_verbs': [\"chases\", \"flees\", \"bites\", \"eats\"],\n",
    "    'intransitive_verbs': [\"barks\", \"meows\", \"squeaks\", \"clucks\", \"chases\", \"runs\", \"swims\"],\n",
    "     'prepositions': [\"at\", \"after\", \"in\", \"on\"]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T16:29:34.026609Z",
     "start_time": "2023-05-13T16:29:34.001956Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "def create_training_data(raw_data):\n",
    "    labels, sentences = [], []\n",
    "    for tupl in raw_data:\n",
    "        labels.append([1,0])\n",
    "        labels.append([1,0])\n",
    "        labels.append([1,0])\n",
    "        labels.append([1,0])\n",
    "        labels.append([1,0])\n",
    "        labels.append([1,0])\n",
    "        labels.append([1,0])\n",
    "        sentences.append(tupl[0])\n",
    "        sentences.append(tupl[0])\n",
    "        sentences.append(tupl[0])\n",
    "        sentences.append(tupl[0])\n",
    "        sentences.append(tupl[0])\n",
    "        sentences.append(tupl[0])\n",
    "        sentences.append(tupl[0])\n",
    "        replaced_token = word_tokenize(tupl[0])[int(tupl[1])]\n",
    "        for key, value in grammar_classes.items():\n",
    "            if replaced_token in value:\n",
    "                for word in value:\n",
    "                    if word != replaced_token:\n",
    "                        new_list = word_tokenize(tupl[0])\n",
    "                        new_list[word_tokenize(tupl[0]).index(replaced_token)] = word\n",
    "                        new_sen = ' '.join(new_list)\n",
    "                        labels.append([0,1])\n",
    "                        sentences.append(new_sen)\n",
    "    return labels, sentences\n",
    "training_labels, training_data = create_training_data(raw_train_data)\n",
    "dev_labels, dev_data = create_training_data(raw_dev_data)\n",
    "#test_labels, test_data = create_training_data(raw_test_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T16:29:34.525163Z",
     "start_time": "2023-05-13T16:29:34.026989Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from discopy import Functor, Ty, Word, Id, Cup, Diagram\n",
    "\n",
    "s, n = Ty('s'), Ty('n')\n",
    "tv = n.r @ s @ n.l\n",
    "iv = n.r @ s\n",
    "p = s.r @ n.r.r @ n.r @ s @ n.l\n",
    "\n",
    "# grammars\n",
    "grammars = [Cup(n, n.r) @ Id(s),\n",
    "            Cup(n, n.r) @ Id(s) @ Cup(n.l, n),\n",
    "            (Id(n) @ Id(n.r) @ Cup(s, s.r) @ Id(n.r.r) @ Id(n.r) >> Id(n) @ Cup(n.r, n.r.r) @ Id(n.r) >> Cup(n, n.r)) @ Id(s) @ Cup(n.l, n)]\n",
    "\n",
    "class WordBox(Word):\n",
    "    \"\"\" Word with Ty((name, cod)) as domain. \"\"\"\n",
    "    def __init__(self, name, cod):\n",
    "        super().__init__(name, cod, dom=Ty())\n",
    "\n",
    "def parse(dataset):\n",
    "    diagram_list = []\n",
    "    for tup in dataset:\n",
    "        sen = tup\n",
    "        tokenized_sen = word_tokenize(sen)\n",
    "        lenght = len(tokenized_sen)\n",
    "        if lenght == 2:\n",
    "            subj_box = WordBox(tokenized_sen[0], n)\n",
    "            verb_box = WordBox(tokenized_sen[1], iv)\n",
    "            diagram = subj_box @ verb_box >> grammars[0]\n",
    "        elif lenght == 3:\n",
    "            subj_box = WordBox(tokenized_sen[0], n)\n",
    "            verb_box = WordBox(tokenized_sen[1], tv)\n",
    "            dobj_box = WordBox(tokenized_sen[2], n)\n",
    "            diagram = subj_box @ verb_box @ dobj_box >> grammars[1]\n",
    "        else:\n",
    "            subj_box = WordBox(tokenized_sen[0], n)\n",
    "            verb_box = WordBox(tokenized_sen[1], iv)\n",
    "            prep_box = WordBox(tokenized_sen[2], p)\n",
    "            idobj_box = WordBox(tokenized_sen[3], n)\n",
    "            diagram = subj_box @ verb_box @ prep_box @ idobj_box >> grammars[2]\n",
    "        diagram_list.append(diagram)\n",
    "    return diagram_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T16:29:34.553231Z",
     "start_time": "2023-05-13T16:29:34.528186Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "uncuped_train_diagrams = parse(training_data)\n",
    "uncuped_dev_diagrams = parse(dev_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T16:29:35.265105Z",
     "start_time": "2023-05-13T16:29:34.576130Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from lambeq import remove_cups\n",
    "\n",
    "train_diagrams = [remove_cups(diagram) for diagram in uncuped_train_diagrams]\n",
    "dev_diagrams = [remove_cups(diagram) for diagram in uncuped_dev_diagrams]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T16:29:37.117378Z",
     "start_time": "2023-05-13T16:29:35.266140Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# in this ansatz block, alot of parameters can be changed\n",
    "from lambeq import AtomicType, IQPAnsatz\n",
    "qubit_map = {t: 1 for t in AtomicType}\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "P = AtomicType.PREPOSITIONAL_PHRASE\n",
    "ansatz = IQPAnsatz(ob_map={N: 1, S: 1, P: 1},\n",
    "                   n_layers=4, n_single_qubit_params=8)\n",
    "\n",
    "train_circuits = [ansatz(diagram) for diagram in train_diagrams]\n",
    "dev_circuits = [ansatz(diagram) for diagram in dev_diagrams]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T16:30:10.206150Z",
     "start_time": "2023-05-13T16:29:37.136440Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from pytket.extensions.qiskit import IBMQBackend, AerBackend\n",
    "from lambeq import TketModel\n",
    "\n",
    "all_circuits = train_circuits + dev_circuits\n",
    "#+test_circuits\n",
    "\n",
    "backend = AerBackend\n",
    "backend_config = {\n",
    "    'backend': backend,\n",
    "    'compilation': backend.default_compilation_pass(2),\n",
    "    'shots': 1024\n",
    "}\n",
    "\n",
    "model = TketModel.from_diagrams(all_circuits, backend_config=backend_config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T16:35:21.141113Z",
     "start_time": "2023-05-13T16:35:06.065911Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# another import from sklearn, may contain large files\n",
    "from sklearn.metrics import hinge_loss as hinge_sk\n",
    "\n",
    "# different loss functions, hinge loss outperforms binary cross entropy loss\n",
    "def hinge_loss(y_hat, y):\n",
    "    return hinge_sk(y[0], y_hat[0])\n",
    "\n",
    "def bce_loss(y_hat, y):\n",
    "    return -np.sum(y * np.log(y_hat)) / len(y)\n",
    "\n",
    "def acc(y_hat, y):\n",
    "    return np.sum(np.round(y_hat) == y) / len(y) / 2\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T16:35:38.984682Z",
     "start_time": "2023-05-13T16:35:38.979266Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from lambeq import QuantumTrainer, SPSAOptimizer\n",
    "\n",
    "trainer = QuantumTrainer(\n",
    "    model,\n",
    "    loss_function=hinge_loss,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=SPSAOptimizer,\n",
    "    optim_hyperparams={'a': 0.05, 'c': 0.06, 'A':0.01*EPOCHS},\n",
    "    evaluate_functions={'acc': acc},\n",
    "    evaluate_on_train=True,\n",
    "    verbose = 'text',\n",
    "    seed=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T16:35:39.518669Z",
     "start_time": "2023-05-13T16:35:39.516020Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from lambeq import Dataset\n",
    "\n",
    "train_dataset = Dataset(\n",
    "            train_circuits,\n",
    "            training_labels,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True)\n",
    "\n",
    "val_dataset = Dataset(dev_circuits, dev_labels, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T16:35:40.076365Z",
     "start_time": "2023-05-13T16:35:40.072091Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n",
      "Job status is QUEUED\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogging_step\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/lambeq/training/quantum_trainer.py:199\u001B[0m, in \u001B[0;36mQuantumTrainer.fit\u001B[0;34m(self, train_dataset, val_dataset, evaluation_step, logging_step)\u001B[0m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    192\u001B[0m         train_dataset: Dataset,\n\u001B[1;32m    193\u001B[0m         val_dataset: Dataset \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    194\u001B[0m         evaluation_step: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m    195\u001B[0m         logging_step: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    197\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39m_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevaluation_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogging_step\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39m_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/lambeq/training/trainer.py:375\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, train_dataset, val_dataset, evaluation_step, logging_step)\u001B[0m\n\u001B[1;32m    373\u001B[0m step \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    374\u001B[0m x, y_label \u001B[38;5;241m=\u001B[39m batch\n\u001B[0;32m--> 375\u001B[0m y_hat, loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate_on_train\n\u001B[1;32m    377\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate_functions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    378\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m metr, func \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate_functions\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/lambeq/training/quantum_trainer.py:163\u001B[0m, in \u001B[0;36mQuantumTrainer.training_step\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Perform a training step.\u001B[39;00m\n\u001B[1;32m    150\u001B[0m \n\u001B[1;32m    151\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    160\u001B[0m \n\u001B[1;32m    161\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39m_clear_predictions()\n\u001B[0;32m--> 163\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    164\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39m_train_predictions[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_costs\u001B[38;5;241m.\u001B[39mappend(loss)\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/lambeq/training/spsa_optimizer.py:141\u001B[0m, in \u001B[0;36mSPSAOptimizer.backward\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    139\u001B[0m xplus \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproject(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mck \u001B[38;5;241m*\u001B[39m delta)\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mweights \u001B[38;5;241m=\u001B[39m xplus\n\u001B[0;32m--> 141\u001B[0m y0 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdiagrams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    142\u001B[0m loss0 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn(y0, targets)\n\u001B[1;32m    144\u001B[0m xminus \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproject(x \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mck \u001B[38;5;241m*\u001B[39m delta)\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/lambeq/training/quantum_model.py:146\u001B[0m, in \u001B[0;36mQuantumModel.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 146\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_training:\n\u001B[1;32m    148\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log_prediction(out)\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/lambeq/training/tket_model.py:133\u001B[0m, in \u001B[0;36mTketModel.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: \u001B[38;5;28mlist\u001B[39m[Diagram]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[1;32m    116\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Perform default forward pass of a lambeq quantum model.\u001B[39;00m\n\u001B[1;32m    117\u001B[0m \n\u001B[1;32m    118\u001B[0m \u001B[38;5;124;03m    In case of a different datapoint (e.g. list of tuple) or\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    131\u001B[0m \n\u001B[1;32m    132\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_diagram_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/lambeq/training/tket_model.py:102\u001B[0m, in \u001B[0;36mTketModel.get_diagram_output\u001B[0;34m(self, diagrams)\u001B[0m\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWeights and/or symbols not initialised. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     96\u001B[0m                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInstantiate through \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     97\u001B[0m                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`TketModel.from_diagrams()` first, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     98\u001B[0m                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthen call `initialise_weights()`, or load \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     99\u001B[0m                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfrom pre-trained checkpoint.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    101\u001B[0m lambdified_diagrams \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_lambda(d) \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m diagrams]\n\u001B[0;32m--> 102\u001B[0m tensors \u001B[38;5;241m=\u001B[39m \u001B[43mCircuit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    103\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mdiag_f\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdiag_f\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlambdified_diagrams\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackend_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_randint\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbackend\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[1;32m    108\u001B[0m \u001B[38;5;66;03m# discopy evals a single diagram into a single result\u001B[39;00m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;66;03m# and not a list of results\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/discopy/quantum/circuit.py:288\u001B[0m, in \u001B[0;36mCircuit.eval\u001B[0;34m(self, backend, mixed, contractor, *others, **params)\u001B[0m\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(box)(box\u001B[38;5;241m.\u001B[39mdom, box\u001B[38;5;241m.\u001B[39mcod, box\u001B[38;5;241m.\u001B[39marray \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m0\u001B[39mj)\n\u001B[1;32m    287\u001B[0m circuits \u001B[38;5;241m=\u001B[39m [circuit\u001B[38;5;241m.\u001B[39mto_tk() \u001B[38;5;28;01mfor\u001B[39;00m circuit \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28mself\u001B[39m, ) \u001B[38;5;241m+\u001B[39m others]\n\u001B[0;32m--> 288\u001B[0m results, counts \u001B[38;5;241m=\u001B[39m [], \u001B[43mcircuits\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_counts\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcircuits\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbackend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbackend\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, circuit \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(circuits):\n\u001B[1;32m    291\u001B[0m     n_bits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(circuit\u001B[38;5;241m.\u001B[39mpost_processing\u001B[38;5;241m.\u001B[39mdom)\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/discopy/quantum/tk.py:136\u001B[0m, in \u001B[0;36mCircuit.get_counts\u001B[0;34m(self, backend, *others, **params)\u001B[0m\n\u001B[1;32m    133\u001B[0m         compilation\u001B[38;5;241m.\u001B[39mapply(circuit)\n\u001B[1;32m    134\u001B[0m handles \u001B[38;5;241m=\u001B[39m backend\u001B[38;5;241m.\u001B[39mprocess_circuits(\n\u001B[1;32m    135\u001B[0m     (\u001B[38;5;28mself\u001B[39m, ) \u001B[38;5;241m+\u001B[39m others, n_shots\u001B[38;5;241m=\u001B[39mn_shots, seed\u001B[38;5;241m=\u001B[39mseed)\n\u001B[0;32m--> 136\u001B[0m counts \u001B[38;5;241m=\u001B[39m [backend\u001B[38;5;241m.\u001B[39mget_result(h)\u001B[38;5;241m.\u001B[39mget_counts() \u001B[38;5;28;01mfor\u001B[39;00m h \u001B[38;5;129;01min\u001B[39;00m handles]\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m normalize:\n\u001B[1;32m    138\u001B[0m     counts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(probs_from_counts, counts))\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/discopy/quantum/tk.py:136\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    133\u001B[0m         compilation\u001B[38;5;241m.\u001B[39mapply(circuit)\n\u001B[1;32m    134\u001B[0m handles \u001B[38;5;241m=\u001B[39m backend\u001B[38;5;241m.\u001B[39mprocess_circuits(\n\u001B[1;32m    135\u001B[0m     (\u001B[38;5;28mself\u001B[39m, ) \u001B[38;5;241m+\u001B[39m others, n_shots\u001B[38;5;241m=\u001B[39mn_shots, seed\u001B[38;5;241m=\u001B[39mseed)\n\u001B[0;32m--> 136\u001B[0m counts \u001B[38;5;241m=\u001B[39m [\u001B[43mbackend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mget_counts() \u001B[38;5;28;01mfor\u001B[39;00m h \u001B[38;5;129;01min\u001B[39;00m handles]\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m normalize:\n\u001B[1;32m    138\u001B[0m     counts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(probs_from_counts, counts))\n",
      "File \u001B[0;32m~/PycharmProjects/discopy_0.5.0/lib/python3.10/site-packages/pytket/extensions/qiskit/backends/ibm.py:581\u001B[0m, in \u001B[0;36mIBMQBackend.get_result\u001B[0;34m(self, handle, **kwargs)\u001B[0m\n\u001B[1;32m    579\u001B[0m             status \u001B[38;5;241m=\u001B[39m job\u001B[38;5;241m.\u001B[39mstatus()\n\u001B[1;32m    580\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJob status is\u001B[39m\u001B[38;5;124m\"\u001B[39m, status\u001B[38;5;241m.\u001B[39mname)\n\u001B[0;32m--> 581\u001B[0m             \u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    583\u001B[0m     res \u001B[38;5;241m=\u001B[39m job\u001B[38;5;241m.\u001B[39mresult(timeout\u001B[38;5;241m=\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    584\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m circ_index, (r, d) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mzip\u001B[39m(res\u001B[38;5;241m.\u001B[39mquasi_dists, res\u001B[38;5;241m.\u001B[39mmetadata)):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.fit(train_dataset, val_dataset, logging_step=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T17:10:25.052378Z",
     "start_time": "2023-05-13T16:35:40.781822Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
